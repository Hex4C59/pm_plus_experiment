# metrics 模板说明

此目录为“评估指标（metrics）”代码模板，负责组织项目中用于验证与评估模型性能的各类指标实现与组合器。README 只说明目录定位、接口约定与使用方式，不涉及具体实现细节或文件内容。

目的
- 集中管理所有评估指标实现，便于复用、对比与替换。
- 约定统一的指标接口，降低评估/日志代码对具体实现的耦合。
- 支持单次批量评估以及多轮/分布式聚合（必要时）以保证评估结果一致性。

建议目录组织
- metrics/
  - __init__.py          # 可选：导出常用指标或注册器入口
  - base.py              # 可选：指标基类或接口定义（包含聚合、重置等通用方法）
  - <task>_metric.py     # 占位：按任务或指标分类的实现文件（例如 accuracy、iou、f1）
  - factory.py           # 可选：指标构造器/注册器，根据名称创建指标实例
  - utils.py             # 可选：指标辅助函数（如同步、累积、格式化）
  - tests/               # 可选：指标单元测试（确保数值与接口正确）
  - README.md            # 本文件

接口约定（建议）
- 单个指标应为可调用对象或类，签名示例：
  - metric = MetricClass(**cfg)
  - value = metric(predictions, targets, mask=None)
- 返回值建议为标量或字典（例如 {'metric': value, 'per_class': [...] }），并提供 reset()/update()/compute() 方法以支持累积式评估：
  - metric.update(preds, targets)
  - result = metric.compute()
  - metric.reset()
- 指标实现应明确是否支持反向传播（大多数评估指标不需要可微），并在文档/接口中标注。
- 在分布式评估场景下，建议提供可选的同步/聚合钩子或工具函数。

使用与扩展
- 新增指标时，请遵循统一签名并在 factory 或 __init__ 中注册/导出，便于评估脚本按名称加载。
- 将指标相关的超参（阈值、忽略类别、平均方式等）暴露在构造函数中，便于配置化管理。
- 避免在指标实现中耦合训练循环、优化器或日志逻辑；仅负责计算与（可选的）聚合。

测试与验证
- 为每个指标添加单元测试，覆盖输出形状、边界条件和已知小例子的数值正确性。
- 在多类别/不平衡数据场景下，测试平均方式与忽略类别逻辑是否按预期工作。
- 在需要分布式聚合的指标上，测试同步逻辑与数值一致性。

维护提示
- 保持实现简洁、纯粹（仅负责评估），将日志、可视化和存储等功能放在上层流程中。
- 文档化指标的数学定义、期望输入/输出格式与限制，便于团队成员使用与复现。
- 优先使用确定性实现以便复现评估结果；对随机或近似算法需明确标注并提供可控的随机种子。